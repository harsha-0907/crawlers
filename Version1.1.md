# Crawler

## Objectives
1. Employ Depth First Search to control the depth of crawl (default-> 4)
2. Proper Logging to ensure that the status of the scan is available (default -> DEBUG)
3. Proper Flow-charts, Notes & Diagrams to explain the working & reasoning for the code 
4. Assign weights to urls to ensure that priority can be given
5. Improve the efficiency & speed of the scan.

## Developer-Notes
1. Create custome Exceptions so that we need not import exceptions from all over the place
